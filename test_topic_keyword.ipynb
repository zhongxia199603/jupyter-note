{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 从文件中随机抽取15个样例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "490000\n"
     ]
    }
   ],
   "source": [
    "with open('/home/jinhanqi/tf_program/topic2essay_data/composition.txt', 'r') as f:\n",
    "    con = f.readlines()\n",
    "print len(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "地球 上 的 光明 和 温暖 ， 都 是 我 送来 的 。 没有 了 我 ， 地球 上 会 一片 凄凉 ， 没有 风 、 雪 、 雨 、 露 ， 也 没有 草 、 木 、 鸟 、 兽 ， 你们 人类 即使 生存 在 地球 上 也 不会 生存 多久 啊 ！ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print con[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "random.shuffle(con)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "表壳 里 ， 最长 的 针 是 秒针 ， 最短 的 是 时针 ， 而 那根 不长 也 不断 的 就是 分针 喽 ！ 时间 一分一秒 的 过去 了 。 俗话说 ： 一寸光阴一寸金 ， 寸金难买 寸 光阴 。 快快 戴 上 我 ， 让 我 告诉 你 ， 你 又 虚费 了 多少 光阴 ， 又 把握 了 多少 时间 吧 ！ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print con[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = con[0:15]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "articles = [s.split('</d>')[0] for s in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "表壳 里 ， 最长 的 针 是 秒针 ， 最短 的 是 时针 ， 而 那根 不长 也 不断 的 就是 分针 喽 ！ 时间 一分一秒 的 过去 了 。 俗话说 ： 一寸光阴一寸金 ， 寸金难买 寸 光阴 。 快快 戴 上 我 ， 让 我 告诉 你 ， 你 又 虚费 了 多少 光阴 ， 又 把握 了 多少 时间 吧 ！ \n",
      "\n"
     ]
    }
   ],
   "source": [
    "print articles[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-44-bc1417476382>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mkeywords\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'</d>'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0ms\u001b[0m \u001b[0;32min\u001b[0m \u001b[0msamples\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "keywords = [s.split('</d>')[1] for s in samples]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 头部 妈妈 千奇百怪 裂开 现在\r\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print keywords[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 定义TF-IDF模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import math\n",
    "import jieba\n",
    "import jieba.posseg as psg\n",
    "from gensim import corpora, models\n",
    "from jieba import analyse\n",
    "import functools\n",
    " \n",
    "def get_stopword_list():\n",
    "\tstop_word_path = './/stopwords/stopword.txt'\n",
    "\tstopword_list = [sw.replace('\\n','') for sw in open(stop_word_path).readlines()]\n",
    "\treturn stopword_list\n",
    " \n",
    "# 分词方法\n",
    "def seg_to_list(sentence, pos=False):\n",
    "\tif not pos:\n",
    "\t\t# 不进行词性标注的分词方法\n",
    "\t\tseg_list = jieba.cut(sentence)\n",
    "\telse:\n",
    "\t\t# 进行词性标注的分词方法\n",
    "\t\tseg_list = psg.cut(sentence)\n",
    "\treturn seg_list\n",
    " \n",
    "# 去除干扰词，根据pos判断是否过滤除名词外的其他词性，再判断词是否在停用词表中，长度是否大于等于2等。\n",
    "def word_filter(seg_list, pos=False):\n",
    "\tstopword_list = get_stopword_list()\n",
    "\tfilter_list = []\n",
    "\t# 根据pos参数选择是否词性过滤\n",
    "\t# 不进行词性过滤，则将词性都标记为n,表示全部保留\n",
    "\tfor seg in seg_list:\n",
    "\t\tif not pos:\n",
    "\t\t\tword = seg\n",
    "\t\t\tflag = 'n'\n",
    "\t\telse:\n",
    "\t\t\tword = seg.word\n",
    "\t\t\tflag = seg.flag\n",
    "\t\tif not flag.startswith('n'):\n",
    "\t\t\tcontinue\n",
    "\t\t# 过滤高停用词表中的词，以及长度为<2的词\n",
    "\t\tif not word in stopword_list and len(word)>1:\n",
    "\t\t\tfilter_list.append(word)\n",
    "\treturn filter_list\n",
    " \n",
    " # 数据加载\n",
    " \n",
    "def load_data(pos=False,corpus_path='/home/jinhanqi/tf_program/topic2essay_data/null.txt'):\n",
    " \tdoc_list = []\n",
    " \tfor line in open(corpus_path, 'r'):\n",
    " \t\tcontent = line.strip()\n",
    " \t\tseg_list = seg_to_list(content, pos)\n",
    " \t\tfilter_list = word_filter(seg_list, pos)\n",
    " \t\tdoc_list.append(filter_list)\n",
    " \treturn doc_list\n",
    " \n",
    "# idf值统计方法\n",
    "def train_idf(doc_list):\n",
    "\tidf_dic = {}\n",
    "\t# 总文档数\n",
    "\ttt_count = len(doc_list)\n",
    "\t# 每个词出现的文档数\n",
    "\tfor doc in doc_list:\n",
    "\t\tfor word in set(doc):\n",
    "\t\t\tidf_dic[word] = idf_dic.get(word, 0.0) + 1.0\n",
    " \n",
    "\t# 按公式转换为idf值，分母加1进行平滑处理\n",
    "\tfor k, v in idf_dic.items():\n",
    "\t\tidf_dic[k] = math.log(tt_count/(1.0+v))\n",
    "\t# 对于没有在字典中的词，默认其尽在一个文档出现，得到默认idf值\n",
    "\tdefault_idf = math.log(tt_count/(1.0))\n",
    "\treturn idf_dic, default_idf\n",
    " \n",
    "# topK\n",
    "def cmp(e1, e2):\n",
    "\timport numpy as np \n",
    "\tres = np.sign(e1[1] - e2[1])\n",
    "\tif res != 0:\n",
    "\t\treturn res \n",
    "\telse:\n",
    "\t\ta = e1[0] + e2[0]\n",
    "\t\tb = e2[0] + e1[0]\n",
    "\t\tif a > b:\n",
    "\t\t\treturn 1\n",
    "\t\telif a == b:\n",
    "\t\t\treturn 0\n",
    "\t\telse:\n",
    "\t\t\treturn -1\n",
    " \n",
    "# TF-IDF类\n",
    "class TfIdf(object):\n",
    "\t# 训练好的idf字典，默认idf值，处理后的待提取文本，关键词数量\n",
    "\tdef __init__(self, idf_dic, default_idf, word_list, keyword_num):\n",
    "\t\tself.word_list = word_list\n",
    "\t\tself.idf_dic, self.default_idf = idf_dic, default_idf\n",
    "\t\tself.tf_dic = self.get_tf_dic()\n",
    "\t\tself.keyword_num = keyword_num\n",
    "\t#统计tf值\n",
    "\tdef get_tf_dic(self):\n",
    "\t\ttf_dic = {}\n",
    "\t\tfor word in self.word_list:\n",
    "\t\t\ttf_dic[word] = tf_dic.get(word, 0.0) + 1.0\n",
    "\t\ttt_count = len(self.word_list)\n",
    "\t\tfor k,v in tf_dic.items():\n",
    "\t\t\ttf_dic[k] = float(v)/tt_count\n",
    "\t\treturn tf_dic\n",
    "\t# 按公式计算tf-idf\n",
    "\tdef get_tfidf(self):\n",
    "\t\ttfidf_dic = {}\n",
    "\t\tfor word in self.word_list:\n",
    "\t\t\tidf = self.idf_dic.get(word, self.default_idf)\n",
    "\t\t\ttf = self.tf_dic.get(word, 0)\n",
    "\t\t\ttfidf = tf*idf\n",
    "\t\t\ttfidf_dic[word] = tfidf\n",
    "\t\tres = [] # 根据tf-idf排序，取排名前keyword_num的词作为关键词\n",
    "        \n",
    "\t\tfor k,v in sorted(tfidf_dic.items(), key=functools.cmp_to_key(cmp), reverse=True)[:self.keyword_num]:\n",
    "\t\t\tres.append(k)\n",
    "\t\treturn res\n",
    " \n",
    "# 主题模型\n",
    "class TopicModel(object):\n",
    "\t# \n",
    "\tdef __init__(self, doc_list, keyword_num, model=\"LSI\", num_topics=4):\n",
    "\t\t# 使用gensim接口，将文本转为向量化表示\n",
    "\t\tself.dictionary = corpora.Dictionary(doc_list)\n",
    "\t\t# 使用BOW模型向量化\n",
    "\t\tcorpus = [self.dictionary.doc2bow(doc) for doc in doc_list]\n",
    "\t\t# 对每个词，根据tf-idf进行加权，得到加权后的向量表示\n",
    "\t\tself.tfidf_model = models.TfidfModel(corpus)\n",
    "\t\tself.corpus_tfidf = self.tfidf_model[corpus]\n",
    "\t\t\n",
    "\t\tself.keyword_num = keyword_num\n",
    "\t\tself.num_topics = num_topics\n",
    "\t\t# 选择加载的模型\n",
    "\t\tif model == 'LSI':\n",
    "\t\t\tself.model = self.train_lsi()\n",
    "\t\telse:\n",
    "\t\t\tself.model = self.train_lda()\n",
    "\t\t# 得到数据集的主题-词分布\n",
    "\t\tword_dic = self.word_dictionary(doc_list)\n",
    "\t\tself.wordtopic_dic = self.get_wordtopic(word_dic)\n",
    " \n",
    "\tdef train_lsi(self):\n",
    "\t\tlsi = models.LsiModel(self.corpus_tfidf, id2word=self.dictionary, num_topics=self.num_topics)\n",
    "\t\treturn lsi\n",
    " \n",
    "\tdef train_lda(self):\n",
    "\t\tlda = models.LdaModel(self.corpus_tfidf, id2word=self.dictionary, num_topics=self.num_topics)\n",
    "\t\treturn lda\n",
    " \n",
    "\tdef get_wordtopic(self, word_dic):\n",
    "\t\twordtopic_dic = {}\n",
    "\t\tfor word in word_dic:\n",
    "\t\t\tsingle_list = [word]\n",
    "\t\t\twordcorpus = self.tfidf_model[self.dictionary.doc2bow(single_list)]\n",
    "\t\t\twordtopic = self.model[wordcorpus]\n",
    "\t\t\twordtopic_dic[word] = wordtopic\n",
    "\t\treturn wordtopic_dic\n",
    " \n",
    "    # 词空间构建方法和向量化方法，在没有gensim接口时的一般处理方法\n",
    "\tdef word_dictionary(self, doc_list):\n",
    "\t\tdictionary = []\n",
    "\t\tfor doc in doc_list:\n",
    "\t\t\tdictionary.extend(doc)\n",
    " \n",
    "\t\tdictionary = list(set(dictionary))\n",
    " \n",
    "\t\treturn dictionary\n",
    " \n",
    "\tdef doc2bowvec(self, word_list):\n",
    "\t\tvec_list = [1 if word in word_list else 0 for word in self.dictionary]\n",
    "\t\treturn vec_list\n",
    " \n",
    "\t# 计算词的分布和文档的分布的相似度，取相似度最高的keyword_num个词作为关键词\n",
    "\tdef get_simword(self, word_list):\n",
    "\t\tsentcorpus = self.tfidf_model[self.dictionary.doc2bow(word_list)]\n",
    "\t\tsenttopic = self.model[sentcorpus]\n",
    "\t\t# 余弦相似度计算\n",
    "\t\tdef calsim(l1, l2):\n",
    "\t\t\ta, b, c = 0.0, 0.0, 0.0\n",
    "\t\t\tfor t1, t2 in zip(l1, l2):\n",
    "\t\t\t\tx1 = t1[1]\n",
    "\t\t\t\tx2 = t2[1]\n",
    "\t\t\t\ta += x1 * x1\n",
    "\t\t\t\tb += x1 * x1\n",
    "\t\t\t\tc += x2 * x2\n",
    "\t\t\tsim = a / math.sqrt(b * c) if not (b * c) == 0.0 else 0.0\n",
    "\t\t\treturn sim\n",
    "\t\t# 计算输入文本和每个词的主题分布相似度\n",
    "\t\tsim_dic = {}\n",
    "\t\tfor k,v in self.wordtopic_dic.items():\n",
    "\t\t\tif k not in word_list:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tsim = calsim(v, senttopic)\n",
    "\t\t\tsim_dic[k] = sim\n",
    " \n",
    "\t\tfor k,v in sorted(sim_dic.items(), key=functools.cmp_to_key(cmp), reverse=True)[:self.keyword_num]:\n",
    "\t\t\tprint(k)\n",
    "\t\tprint \"\"\n",
    " \n",
    " \n",
    "def tfidf_extract(word_list, pos=False, keyword_num=10):\n",
    "\tdoc_list = load_data(pos)\n",
    "\tidf_dic, default_idf = train_idf(doc_list)\n",
    "\ttfidf_model = TfIdf(idf_dic, default_idf, word_list, keyword_num)\n",
    "\treturn tfidf_model.get_tfidf()\n",
    " \n",
    "def textrank_extract(text, pos=False, keyword_num=10):\n",
    "\ttextrank = analyse.textrank\n",
    "\tkeywords = textrank(text, keyword_num)\n",
    "\t# 输出抽取出的关键词\n",
    "\tfor keyword in keywords:\n",
    "\t\tprint(keyword)\n",
    "\t#print()\n",
    " \n",
    "def topic_extract(word_list, model, pos=False, keyword_num=10):\n",
    "\tdoc_list = load_data(pos)\n",
    "\ttopic_model = TopicModel(doc_list, keyword_num, model=model)\n",
    "\ttopic_model.get_simword(word_list)\n",
    "    \n",
    "def getTFIDF(text):\n",
    "    pos = True\n",
    "    seg_list = seg_to_list(text, pos)\n",
    "    filter_list = word_filter(seg_list, pos)\n",
    "    return tfidf_extract(filter_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 得到关键词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinhanqi/.local/lib/python2.7/site-packages/ipykernel_launcher.py:40: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "keywords_TF = [getTFIDF(text) for text in articles]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def printChinese(text):\n",
    "    print json.dumps(text, ensure_ascii = False, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[\"部位\", \"妈妈\", \"头部\"]\n"
     ]
    }
   ],
   "source": [
    "printChinese(keywords_TF[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "可 这些 我 虽然 画 得 很 好 ， 可 我 一直 想 画 一个 人 。 尤其 是 我 辛辛苦苦 的 妈妈 。 但是 ， 现在 ， 我 只 会 画 头部 ， 其它 部位 画 得 千奇百怪 。 一些 斜 着 ， 一些 还 裂开 了 ， 画 不成 妈妈 。 \n",
      " 头部 妈妈 千奇百怪 裂开 现在\r\n",
      "\n",
      "[\"部位\", \"妈妈\", \"头部\"]\n",
      "\n",
      "\n",
      "听 了 妈妈 的话 ， 小云 放开 了 手中 的 菊花 ， 不好意思 地 说 ： “ 对不起 ， 妈妈 ， 我 知道 错 了 。 ” 妈妈 摸 着 小云 的 头 说 ： “ 我 的 好孩子 ， 我们 一起 做 个 爱护 环境 的 人 。 ” \n",
      " 妈妈 爱护 环境 知道 对不起\r\n",
      "\n",
      "[\"菊花\", \"环境\", \"小云\", \"妈妈\", \"好孩子\"]\n",
      "\n",
      "\n",
      "小 池塘 的 旁边 还有 许多 小山 和 大树 ， 茂密 的 树叶 好像 搭起 了 一个个 凉棚 。 小 池塘 清澈见底 ， 就连 在 水底 的 细小 沙粒 和 青 褐色 的 石头 都 看 得 一清二楚 。 水底 有 许多 小 金鱼 和 小蝌蚪 ， 每个 小蝌蚪 都 像 一个个 黑色 的 小 逗号 。 \n",
      " 好像 褐色 细小 池塘 金鱼\r\n",
      "\n",
      "[\"黑色\", \"金鱼\", \"逗号\", \"褐色\", \"细小\", \"石头\", \"沙粒\", \"池塘\", \"水底\", \"树叶\"]\n",
      "\n",
      "\n",
      "边检 员 做事 要 谨慎 ， 认真 ， 镇静 ， 反应 要 快 ， 这些 余翔 都 具备 了 。 看 完 这部 电影 ， 我 在 想 ： 世界 上 还有 什么 事 能比 边检 要 更加 谨慎 ， 认真 ， 镇静 ， 反应 要 快 的 。 此时此刻 我 对 他们 肃然起敬 。 \n",
      " 此时此刻 电影 世界 做事 反应\r\n",
      "\n",
      "[\"电影\", \"余翔\", \"世界\"]\n",
      "\n",
      "\n",
      "“ 我 来 ！ ” 我 说 。 “ 啊 ？ 你 ？ 哼 ， 你 骗人 吧 ！ 女生 没有 一个 爬 得 上去 的 ！ 我们 劝 你 还是 放弃 吧 ！ ” 听 着 那 刺激 我 内心 的 话语 ， 我 深深 的 伤心 了 ， 当 我 准备 放弃 时 ， 我 想到 了 父亲 曾 告诫 我 的话 ： 完成 一件 事 ， 就要 坚持到底 ， 不能 半途而废 。 于是 ， 我 饱含 着 伤人 的 话语 ， 走到 了 单杠 面前 。 \n",
      " 放弃 话语 不能 想到 准备\r\n",
      "\n",
      "[\"骗人\", \"话语\", \"父亲\", \"女生\", \"单杠\", \"内心\", \"伤心\", \"伤人\"]\n",
      "\n",
      "\n",
      "我 把 小 螳螂 的 “ 临时 住所 ” 立 起来 ， 从 上面 的 一个 洞 往 里 看 ： 它 正在 青草 上 爬来爬去 ， 急 操 不安 ， 估计 是 在 想 办法 逃出去 ， 可惜 我 挖 的 洞 太小 ， 它 爬 不 出来 。 小 螳螂 可 没 罢休 ， 继续 上蹿下跳 。 \n",
      " 正在 螳螂 继续 太小 罢休\r\n",
      "\n",
      "[\"青草\", \"螳螂\", \"太小\", \"办法\", \"住所\"]\n",
      "\n",
      "\n",
      "好 小 好 小 的 时候 ， 我 就 有 一个 梦想 。 长大 要 当 一名 人民警察 。 当时 的 我 只 知道 做 警察 很 威风 ， 小朋友 都 怕 警察 叔叔 。 等 我 上学 的 时候 我 知道 了 当 警察 的 不 容易 ， 但 我 还是 希望 自己 能 好好学习 ， 以后 能 成为 位 出色 的 人民警察 。 \n",
      " 警察 知道 威风 小朋友 希望\r\n",
      "\n",
      "[\"长大\", \"梦想\", \"时候\", \"小朋友\", \"威风\", \"好好学习\", \"叔叔\", \"人民警察\", \"上学\"]\n",
      "\n",
      "\n",
      "当 你 正 沉迷于 玫瑰花 的 幻想 中 ， 此时 ， 一缕缕 幽香 扑鼻而来 ， 使 人 神清气爽 ， 原来 是 娇小 的 粉 桃花 吸引 着 你 ， 它们 象 小姑娘 一样 藏 在 翠绿 的 叶 里 ， 盛开 的 粉 桃花 像 穿着 纱裙 的 花仙子 ， 在 深绿 的 舞台 上 尽情 歌舞 。 \n",
      " 桃花 玫瑰花 幽香 翠绿 扑鼻而来\r\n",
      "\n",
      "[\"花仙子\", \"舞台\", \"翠绿\", \"纱裙\", \"玫瑰花\", \"歌舞\", \"桃花\", \"幽香\", \"幻想\", \"小姑娘\"]\n",
      "\n",
      "\n",
      "5 月 6 日 ， 我 今天 再 去 看 葫芦 苗 ， 发现 小苗 的 很多 地方 都 打着 花骨朵 了 ， 有 的 甚至 已经 开花 了 ， 小小的 黄花 ， 有点像 南瓜 花 ， 好 漂亮 ！ \n",
      " 有点像 南瓜 发现 葫芦 打着\r\n",
      "\n",
      "[\"黄花\", \"葫芦\", \"花骨朵\", \"开花\", \"小苗\", \"地方\", \"南瓜\"]\n",
      "\n",
      "\n",
      "走出 了 芒果 地 我们 来到 了 石榴 地 ， 映 在 我们 眼里 是 另一番 美景 。 一颗颗 石榴树 花枝招展 ， 好像 是 一个个 活泼 的 小女孩 ， 绽放 着 灿烂 的 笑脸 欢迎 我们 的 到来 。 叶子 碧绿 得 像 无暇 的 美玉 。 在 绿叶 的 衬托 下 ， 白色 的 石榴花 显得 更加 婀娜多姿 。 石榴树 散发 着 阵阵 清香 ， 引来 了 一群 蝴蝶 和 辛勤 的 小蜜蜂 ， 它们 围着 石榴花 时而 翩翩起舞 ， 时而 轻盈 落下 ， 细心地 采集 花粉 。 看到 这 动人 的 情景 ， 我们 如痴如醉 。 \n",
      " 细心地 采集 动人 情景 芒果\r\n",
      "\n",
      "[\"辛勤\", \"衬托\", \"蝴蝶\", \"花粉\", \"芒果\", \"美玉\", \"美景\", \"绿叶\", \"细心地\", \"笑脸\"]\n",
      "\n",
      "\n",
      "窗外 小雨 淅淅沥沥 地下 着 ， 远远望去 一片 灰蒙蒙 的 天空 像 笼上 了 一层 淡淡的 轻 烟 ， 桌上 ， 几本 零乱 的 书 ， 右手 旋转 着 一支 圆珠笔 ， 伴 着 一个 孤单 的 身影 。 夜 ， 寂寞 如 斯 ， 灯 ， 孤单 如 我 。 \n",
      " 右手 天空 圆珠笔 旋转 小雨\r\n",
      "\n",
      "[\"身影\", \"小雨\", \"天空\", \"圆珠笔\", \"右手\"]\n",
      "\n",
      "\n",
      "活动 在 愉快 的 进行 着 ， 同学 们 都 向 大家 介绍 了 自己 名字 的 魅力 。 充满 自豪 ， 充满 自信 ， 是 啊 ！ 父母 为 我们 每个 子女 ， 都 取 了 蕴含 深意 的 名字 ， 他们 把 幻想 、 希望 、 都 寄托 在 儿女 的 名字 上 ， 作为 子女 ， 我们 要 从中 汲取 力量 ， 不 辜负 父母 的 希望 和 重托 ， 做 一个 “ 名副其实 ” 的 人 。 \n",
      " 名字 希望 蕴含 父母 介绍\r\n",
      "\n",
      "[\"魅力\", \"自豪\", \"父母\", \"幻想\", \"子女\", \"大家\", \"名字\", \"同学\", \"力量\", \"儿女\"]\n",
      "\n",
      "\n",
      "现在 ， 我 觉得 姑母 姑父 家 很 温暖 ， 那些 沉默 、 那些 平凡 、 那些 平淡 的 话语 无不 诉说着 温馨 和 关爱 ， 也 让 我 深深地 明白 ： 矛盾 ， 不是 靠 你 争 我 抢来 化解 的 ； 爱 ， 不 在于 嘴 上 ， 而 在于 内心 和 行动 。 \n",
      " 觉得 在于 内心 温馨 明白\r\n",
      "\n",
      "[\"话语\", \"温馨\", \"明白\", \"姑父\", \"姑母\", \"内心\"]\n",
      "\n",
      "\n",
      "其实 ， 考试 是 检验 我们 学习 情况 的 一种 手段 。 掌握 了 学习 方法 和 学习 效率 ， 就 一定 会 有 收获 的 。 我 默默地 想 ： 哪里 跌倒 了 哪里 爬起来 ， 吃一堑 ， 长一智 ， 我 要 做 一个 优秀 的 好 学生 ！ 一次 考试 不好 ， 不 等于 将来 不好 ， 我 不会 就此 放弃 ， 我 要 以下 一次 的 考试 做 目标 ， 争取 考 出 理想 的 成绩 。 \n",
      " 学习 理想 跌倒 爬起来 争取\r\n",
      "\n",
      "[\"长一智\", \"目标\", \"理想\", \"方法\", \"效率\", \"手段\", \"成绩\", \"情况\", \"学生\"]\n",
      "\n",
      "\n",
      "在 上学 、 放学 的 路上 ， 我 总会 看到 一位 塑糖 的 老人 。 老人 坐在 一个 推车 上 , 车上 有 一个 牌子 ， 牌子 上 插 着 “ 仙鹤 ” 、 “ 蝴蝶 ” 、 “ 乌龟 ” 、 “ 龙虾 ” 等用 糖 塑成 的 动物 ， 个个 形象 逼真 ， 栩栩如生 。 \n",
      " 老人 逼真 蝴蝶 乌龟 龙虾\r\n",
      "\n",
      "[\"龙虾\", \"蝴蝶\", \"老人\", \"牌子\", \"放学\", \"总会\", \"形象\", \"塑糖\", \"动物\", \"仙鹤\"]\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for i in range(len(articles)):\n",
    "    print articles[i]\n",
    "    print keywords[i]\n",
    "    printChinese(keywords_TF[i])\n",
    "    print ''\n",
    "    print ''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
