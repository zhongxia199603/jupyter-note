{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "import math\n",
    "import jieba\n",
    "import jieba.posseg as psg\n",
    "from gensim import corpora, models\n",
    "from jieba import analyse\n",
    "import functools\n",
    " \n",
    "def get_stopword_list():\n",
    "\tstop_word_path = './/stopwords/stopword.txt'\n",
    "\tstopword_list = [sw.replace('\\n','') for sw in open(stop_word_path).readlines()]\n",
    "\treturn stopword_list\n",
    " \n",
    "# 分词方法\n",
    "def seg_to_list(sentence, pos=False):\n",
    "\tif not pos:\n",
    "\t\t# 不进行词性标注的分词方法\n",
    "\t\tseg_list = jieba.cut(sentence)\n",
    "\telse:\n",
    "\t\t# 进行词性标注的分词方法\n",
    "\t\tseg_list = psg.cut(sentence)\n",
    "\treturn seg_list\n",
    " \n",
    "# 去除干扰词，根据pos判断是否过滤除名词外的其他词性，再判断词是否在停用词表中，长度是否大于等于2等。\n",
    "def word_filter(seg_list, pos=False):\n",
    "\tstopword_list = get_stopword_list()\n",
    "\tfilter_list = []\n",
    "\t# 根据pos参数选择是否词性过滤\n",
    "\t# 不进行词性过滤，则将词性都标记为n,表示全部保留\n",
    "\tfor seg in seg_list:\n",
    "\t\tif not pos:\n",
    "\t\t\tword = seg\n",
    "\t\t\tflag = 'n'\n",
    "\t\telse:\n",
    "\t\t\tword = seg.word\n",
    "\t\t\tflag = seg.flag\n",
    "\t\tif not flag.startswith('n'):\n",
    "\t\t\tcontinue\n",
    "\t\t# 过滤高停用词表中的词，以及长度为<2的词\n",
    "\t\tif not word in stopword_list and len(word)>1:\n",
    "\t\t\tfilter_list.append(word)\n",
    "\treturn filter_list\n",
    " \n",
    " # 数据加载\n",
    " \n",
    "def load_data(pos=False,corpus_path='/home/jinhanqi/tf_program/topic2essay_data/null.txt'):\n",
    " \tdoc_list = []\n",
    " \tfor line in open(corpus_path, 'r'):\n",
    " \t\tcontent = line.strip()\n",
    " \t\tseg_list = seg_to_list(content, pos)\n",
    " \t\tfilter_list = word_filter(seg_list, pos)\n",
    " \t\tdoc_list.append(filter_list)\n",
    " \treturn doc_list\n",
    " \n",
    "# idf值统计方法\n",
    "def train_idf(doc_list):\n",
    "\tidf_dic = {}\n",
    "\t# 总文档数\n",
    "\ttt_count = len(doc_list)\n",
    "\t# 每个词出现的文档数\n",
    "\tfor doc in doc_list:\n",
    "\t\tfor word in set(doc):\n",
    "\t\t\tidf_dic[word] = idf_dic.get(word, 0.0) + 1.0\n",
    " \n",
    "\t# 按公式转换为idf值，分母加1进行平滑处理\n",
    "\tfor k, v in idf_dic.items():\n",
    "\t\tidf_dic[k] = math.log(tt_count/(1.0+v))\n",
    "\t# 对于没有在字典中的词，默认其尽在一个文档出现，得到默认idf值\n",
    "\tdefault_idf = math.log(tt_count/(1.0))\n",
    "\treturn idf_dic, default_idf\n",
    " \n",
    "# topK\n",
    "def cmp(e1, e2):\n",
    "\timport numpy as np \n",
    "\tres = np.sign(e1[1] - e2[1])\n",
    "\tif res != 0:\n",
    "\t\treturn res \n",
    "\telse:\n",
    "\t\ta = e1[0] + e2[0]\n",
    "\t\tb = e2[0] + e1[0]\n",
    "\t\tif a > b:\n",
    "\t\t\treturn 1\n",
    "\t\telif a == b:\n",
    "\t\t\treturn 0\n",
    "\t\telse:\n",
    "\t\t\treturn -1\n",
    " \n",
    "# TF-IDF类\n",
    "class TfIdf(object):\n",
    "\t# 训练好的idf字典，默认idf值，处理后的待提取文本，关键词数量\n",
    "\tdef __init__(self, idf_dic, default_idf, word_list, keyword_num):\n",
    "\t\tself.word_list = word_list\n",
    "\t\tself.idf_dic, self.default_idf = idf_dic, default_idf\n",
    "\t\tself.tf_dic = self.get_tf_dic()\n",
    "\t\tself.keyword_num = keyword_num\n",
    "\t#统计tf值\n",
    "\tdef get_tf_dic(self):\n",
    "\t\ttf_dic = {}\n",
    "\t\tfor word in self.word_list:\n",
    "\t\t\ttf_dic[word] = tf_dic.get(word, 0.0) + 1.0\n",
    "\t\ttt_count = len(self.word_list)\n",
    "\t\tfor k,v in tf_dic.items():\n",
    "\t\t\ttf_dic[k] = float(v)/tt_count\n",
    "\t\treturn tf_dic\n",
    "\t# 按公式计算tf-idf\n",
    "\tdef get_tfidf(self):\n",
    "\t\ttfidf_dic = {}\n",
    "\t\tfor word in self.word_list:\n",
    "\t\t\tidf = self.idf_dic.get(word, self.default_idf)\n",
    "\t\t\ttf = self.tf_dic.get(word, 0)\n",
    "\t\t\ttfidf = tf*idf\n",
    "\t\t\ttfidf_dic[word] = tfidf\n",
    "\t\t# 根据tf-idf排序，取排名前keyword_num的词作为关键词\n",
    "\t\tfor k,v in sorted(tfidf_dic.items(), key=functools.cmp_to_key(cmp), reverse=True)[:self.keyword_num]:\n",
    "\t\t\tprint(k)\n",
    "\t\tprint ''\n",
    " \n",
    "# 主题模型\n",
    "class TopicModel(object):\n",
    "\t# \n",
    "\tdef __init__(self, doc_list, keyword_num, model=\"LSI\", num_topics=4):\n",
    "\t\t# 使用gensim接口，将文本转为向量化表示\n",
    "\t\tself.dictionary = corpora.Dictionary(doc_list)\n",
    "\t\t# 使用BOW模型向量化\n",
    "\t\tcorpus = [self.dictionary.doc2bow(doc) for doc in doc_list]\n",
    "\t\t# 对每个词，根据tf-idf进行加权，得到加权后的向量表示\n",
    "\t\tself.tfidf_model = models.TfidfModel(corpus)\n",
    "\t\tself.corpus_tfidf = self.tfidf_model[corpus]\n",
    "\t\t\n",
    "\t\tself.keyword_num = keyword_num\n",
    "\t\tself.num_topics = num_topics\n",
    "\t\t# 选择加载的模型\n",
    "\t\tif model == 'LSI':\n",
    "\t\t\tself.model = self.train_lsi()\n",
    "\t\telse:\n",
    "\t\t\tself.model = self.train_lda()\n",
    "\t\t# 得到数据集的主题-词分布\n",
    "\t\tword_dic = self.word_dictionary(doc_list)\n",
    "\t\tself.wordtopic_dic = self.get_wordtopic(word_dic)\n",
    " \n",
    "\tdef train_lsi(self):\n",
    "\t\tlsi = models.LsiModel(self.corpus_tfidf, id2word=self.dictionary, num_topics=self.num_topics)\n",
    "\t\treturn lsi\n",
    " \n",
    "\tdef train_lda(self):\n",
    "\t\tlda = models.LdaModel(self.corpus_tfidf, id2word=self.dictionary, num_topics=self.num_topics)\n",
    "\t\treturn lda\n",
    " \n",
    "\tdef get_wordtopic(self, word_dic):\n",
    "\t\twordtopic_dic = {}\n",
    "\t\tfor word in word_dic:\n",
    "\t\t\tsingle_list = [word]\n",
    "\t\t\twordcorpus = self.tfidf_model[self.dictionary.doc2bow(single_list)]\n",
    "\t\t\twordtopic = self.model[wordcorpus]\n",
    "\t\t\twordtopic_dic[word] = wordtopic\n",
    "\t\treturn wordtopic_dic\n",
    " \n",
    "    # 词空间构建方法和向量化方法，在没有gensim接口时的一般处理方法\n",
    "\tdef word_dictionary(self, doc_list):\n",
    "\t\tdictionary = []\n",
    "\t\tfor doc in doc_list:\n",
    "\t\t\tdictionary.extend(doc)\n",
    " \n",
    "\t\tdictionary = list(set(dictionary))\n",
    " \n",
    "\t\treturn dictionary\n",
    " \n",
    "\tdef doc2bowvec(self, word_list):\n",
    "\t\tvec_list = [1 if word in word_list else 0 for word in self.dictionary]\n",
    "\t\treturn vec_list\n",
    " \n",
    "\t# 计算词的分布和文档的分布的相似度，取相似度最高的keyword_num个词作为关键词\n",
    "\tdef get_simword(self, word_list):\n",
    "\t\tsentcorpus = self.tfidf_model[self.dictionary.doc2bow(word_list)]\n",
    "\t\tsenttopic = self.model[sentcorpus]\n",
    "\t\t# 余弦相似度计算\n",
    "\t\tdef calsim(l1, l2):\n",
    "\t\t\ta, b, c = 0.0, 0.0, 0.0\n",
    "\t\t\tfor t1, t2 in zip(l1, l2):\n",
    "\t\t\t\tx1 = t1[1]\n",
    "\t\t\t\tx2 = t2[1]\n",
    "\t\t\t\ta += x1 * x1\n",
    "\t\t\t\tb += x1 * x1\n",
    "\t\t\t\tc += x2 * x2\n",
    "\t\t\tsim = a / math.sqrt(b * c) if not (b * c) == 0.0 else 0.0\n",
    "\t\t\treturn sim\n",
    "\t\t# 计算输入文本和每个词的主题分布相似度\n",
    "\t\tsim_dic = {}\n",
    "\t\tfor k,v in self.wordtopic_dic.items():\n",
    "\t\t\tif k not in word_list:\n",
    "\t\t\t\tcontinue\n",
    "\t\t\tsim = calsim(v, senttopic)\n",
    "\t\t\tsim_dic[k] = sim\n",
    " \n",
    "\t\tfor k,v in sorted(sim_dic.items(), key=functools.cmp_to_key(cmp), reverse=True)[:self.keyword_num]:\n",
    "\t\t\tprint(k)\n",
    "\t\tprint \"\"\n",
    " \n",
    " \n",
    "def tfidf_extract(word_list, pos=False, keyword_num=10):\n",
    "\tdoc_list = load_data(pos)\n",
    "\tidf_dic, default_idf = train_idf(doc_list)\n",
    "\ttfidf_model = TfIdf(idf_dic, default_idf, word_list, keyword_num)\n",
    "\ttfidf_model.get_tfidf()\n",
    " \n",
    "def textrank_extract(text, pos=False, keyword_num=10):\n",
    "\ttextrank = analyse.textrank\n",
    "\tkeywords = textrank(text, keyword_num)\n",
    "\t# 输出抽取出的关键词\n",
    "\tfor keyword in keywords:\n",
    "\t\tprint(keyword)\n",
    "\t#print()\n",
    " \n",
    "def topic_extract(word_list, model, pos=False, keyword_num=10):\n",
    "\tdoc_list = load_data(pos)\n",
    "\ttopic_model = TopicModel(doc_list, keyword_num, model=model)\n",
    "\ttopic_model.get_simword(word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '赞 流水 ， 思 历史 人物 的 丰功伟绩 ； 瞧 那 奔腾 的 黄河 孕出 的 代代 天骄 ： 秦始皇 统一 中国 之 壮举 ； 唐玄宗 贞观之治 之 大貌 ； 毛泽东 解放 中国 之 雄壮 ； 历史 的 伟人 呀 ， 是 黄河 骄傲 的 子女 ， 是 潺潺流水 不息 的 生命 动力 ；'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = '我 原本 也 是 自卑 者 。 如 一块 石子 ， 默默 的 躺 在 路边 ， 但 感谢 众多 博客 朋友 们 ， 让 我 重拾 了 自信 ， 远离 了 自卑 ， 让 我 明白 ： 即使 自己 是 一块 不起眼 的 石子 ， 也 可以 成为 一块 筑 基石 ， 为 繁忙 的 交通 奉献 了 自己 的 力量 。 李白 不是 曾 自信 的 说 ： “ 天生我才 必有用 ” 吗 ！ 我 也 自信 “ 长风破浪 会 有时 ， 直挂 云帆济 沧海 。 ” 因此 希望 所有 自卑 的 朋友 ― ― 告别 自卑 。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "text ='五 、 如果 您 是 位 特别 讲究 吃 穿 ， 追赶 时髦 的 人 ， 而 这 过分 的 追求 又 常常 弄 得 您 入不敷出 ， 故而 烦恼 不堪 。 那么 请 不要 沮丧 ， “ 卸下 负担 ” 培训班 将 教 您 学会 如何 玩钱 而 不 被 钱 玩弄 ， 如何 做到 收支 平衡 ， 还 您 一个 心情舒畅 的 自我 。 培训 时间 ： 一般 一个月 ， 可 根据 个人 情况 适当 缩短 或 延长 。'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "五 、 如果 您 是 位 特别 讲究 吃 穿 ， 追赶 时髦 的 人 ， 而 这 过分 的 追求 又 常常 弄 得 您 入不敷出 ， 故而 烦恼 不堪 。 那么 请 不要 沮丧 ， “ 卸下 负担 ” 培训班 将 教 您 学会 如何 玩钱 而 不 被 钱 玩弄 ， 如何 做到 收支 平衡 ， 还 您 一个 心情舒畅 的 自我 。 培训 时间 ： 一般 一个月 ， 可 根据 个人 情况 适当 缩短 或 延长 。\n"
     ]
    }
   ],
   "source": [
    "print text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def getTFIDF(text):\n",
    "    pos = True\n",
    "    seg_list = seg_to_list(text, pos)\n",
    "    filter_list = word_filter(seg_list, pos)\n",
    "    return tfidf_extract(filter_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF模型结果：\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinhanqi/.local/lib/python2.7/site-packages/ipykernel_launcher.py:40: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "黄河\n",
      "秦始皇\n",
      "流水\n",
      "毛泽东\n",
      "子女\n",
      "天骄\n",
      "大貌\n",
      "唐玄宗\n",
      "历史\n",
      "动力\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print('TF-IDF模型结果：')\n",
    "getTFIDF(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "中国\n",
      "生命\n",
      "历史\n",
      "黄河\n",
      "孕出\n",
      "动力\n",
      "人物\n",
      "不息\n",
      "伟人\n",
      "奔腾\n"
     ]
    }
   ],
   "source": [
    "textrank_extract(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinhanqi/.local/lib/python2.7/site-packages/ipykernel_launcher.py:40: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "topic_extract(filter_list, 'LSI', pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jinhanqi/.local/lib/python2.7/site-packages/ipykernel_launcher.py:40: UnicodeWarning: Unicode equal comparison failed to convert both arguments to Unicode - interpreting them as being unequal\n"
     ]
    }
   ],
   "source": [
    "topic_extract(filter_list, 'LDA', pos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
